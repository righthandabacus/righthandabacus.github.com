---
layout: post
title: "Gu and Grossman (2007) UDT: UDP-based Data Transfer for High-Speed Wide Area Networks (ComNet)"
date: 2010-05-21 23:17:32 -0400
category: paper
bibtex:
  type: article
  title: "UDT: UDP-based Data Transfer for High-Speed Wide Area Networks"
  author: "Yunhong Gu and Robert L. Grossman"
  journal: "Computer Networks"
  volume: 51
  number: 7
  month: May
  year: 2007
---
Proposed a transport protocol over UDP to better throughput on high BDP network.

UDT is a connection-oriented duplex protocol. Unlike the conventional TCP, it has two mode of connection setup. The client/server mode is like TCP, client sends handshake packet, sender respond, the client can send. The second mode is rendezvous mode, use when both peers are behind firewall. Then both probe for each other, whoever receive the connection request, respond to it. In UDT, there are also mandatory keepalive packets as heartbeat to maintain connection. Connection is closed by sending/receiving shutdown message, or the lost of keepalive.

UDT is reliable protocol. It requires the receiver to ACK all the data received. It is a timer-based selective acknowledgement. For every ACK received, the sender will confirm the ACK with a ACK2. But upon loss is detected, NAK is sent to request for retransmit. The retransmit is also delayed to circumvent out-of-order.

Congestion in UDT is DAIMD, i.e. AIMD with decreasing increments. The increase size is a function of the current rate. The higher the current rate, the smaller the increment. The paper suggested

$$ \alpha(x)=10^{\lceil\log(L-C(x))\rceil-\tau}\dfrac{1500}{S}\dfrac{1}{\textrm{SYN}} $$

to be the increment size at rate $$x$$. Where $$S$$ is the packet size, SYN is the rate control interval, $$L$$ is the estimated link speed in bps, $$C(x)$$ is the current rate in bps. The paper claims that this is stable and fair.

Loss in UDT is handled in different cases. Basically loss triggers rate cut. But to prevent aggressive rate reduction due to synchronization, a randomization method is used: First, the decrease factor is random; Second, rate decrease is done on some NAK only. Namely, when NAK seqno > seqno at the time of last rate cut, or the NAK received is the $$k$$-th one. Another reason for a rate decrease to be too aggressive is that the loss is not due to congestion but due to corruption. This is handled by forgetting the first NAK in a congestion event. This also solves the problem of packet out of order.

This paper did simulation to find the throughput, fairness and stability. The stability formula is the one proposed by the FAST-TCP paper in INFOCOM 2004, i.e.

$$ S=\dfrac{1}{n}\sum_{i=1}^n\big(\dfrac{1}{\bar{x}_i}\sqrt{\frac{1}{m-1}\sum_{k=1}^m(x_i(k)-\bar{x}_i)^2}\big) $$

where $$n$$ is the number of concurrent flows; $$m$$ is the number of throughput samples, $$x(k)$$ is the samples.
